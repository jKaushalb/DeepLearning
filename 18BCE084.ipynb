{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_prac5_RNN_SEquenceModel.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pMVsRKX7F1d"
      },
      "source": [
        "import tensorflow.keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,SimpleRNN,Embedding\n",
        "import sklearn\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "import numpy\n",
        "from numpy import array \n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XL2V8HyPOyeE",
        "outputId": "0aa25e83-7a10-411a-d8d6-ea11048e87f2"
      },
      "source": [
        "data=\"In modern football we are in the After Kaka  era .\\n where conversations about the best in the world evolved from a list of many to a shortlist of two .\\n The game that Messi and Ronaldo have played in the last 10 years is not the same one in which Kaka excelled .\\n The duo moved the sport onwards and demanded everyone else come with them.\"\n",
        "data=data.split('\\n')\n",
        "tk=Tokenizer(filters='')# if filter = '$%' then it will exclude and not consider as token .here in data there is no special char other then . and we want it so it is null \n",
        "tk.fit_on_texts(data) #making the vocab\n",
        "print(\"vocab is :\\n\",tk.word_index)\n",
        "vocab_len=len(tk.word_index)+1 # extra one for handling unknown token\n",
        "print(\"vocab length: \",vocab_len)\n",
        "\n",
        "enSeq=tk.texts_to_sequences(data)\n",
        "print(\"encoded sentences are:\\n\",enSeq)\n",
        "X=[]\n",
        "Y=[]\n",
        "\n",
        "for i in range(len(enSeq)):\n",
        "  X.insert(i,enSeq[i][:-1]) # we do not need '.' in X becuse in last unfolding we only give input till word before '.' \n",
        "  Y.insert(i,enSeq[i])\n",
        "\n",
        "print(\"input:\",X)\n",
        "print(\"output:\",Y)\n",
        "\n",
        "\"\"\"for i in range(len(X)):\n",
        "  #l=0 + X[i]\n",
        "  X[i].insert(0,0)\n",
        "print(\"adding 0 in training:\\n\",X)\"\"\"\n",
        "\n",
        "maxlen=max([len(s) for s in X ])\n",
        "X=pad_sequences(X,maxlen=maxlen+1,padding='pre')\n",
        "print(\"AFTER padding:\",X,\"shape:\\n\",X.shape)\n",
        "Y=pad_sequences(Y,maxlen=maxlen+1,padding='pre')\n",
        "\n",
        "Y=np_utils.to_categorical(Y,num_classes=len(tk.word_index)+1)\n",
        "print(\"one hot vector y\\n\",Y)\n",
        "print(X.shape,Y.shape)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocab is :\n",
            " {'the': 1, 'in': 2, '.': 3, 'kaka': 4, 'a': 5, 'of': 6, 'and': 7, 'modern': 8, 'football': 9, 'we': 10, 'are': 11, 'after': 12, 'era': 13, 'where': 14, 'conversations': 15, 'about': 16, 'best': 17, 'world': 18, 'evolved': 19, 'from': 20, 'list': 21, 'many': 22, 'to': 23, 'shortlist': 24, 'two': 25, 'game': 26, 'that': 27, 'messi': 28, 'ronaldo': 29, 'have': 30, 'played': 31, 'last': 32, '10': 33, 'years': 34, 'is': 35, 'not': 36, 'same': 37, 'one': 38, 'which': 39, 'excelled': 40, 'duo': 41, 'moved': 42, 'sport': 43, 'onwards': 44, 'demanded': 45, 'everyone': 46, 'else': 47, 'come': 48, 'with': 49, 'them.': 50}\n",
            "vocab length:  51\n",
            "encoded sentences are:\n",
            " [[2, 8, 9, 10, 11, 2, 1, 12, 4, 13, 3], [14, 15, 16, 1, 17, 2, 1, 18, 19, 20, 5, 21, 6, 22, 23, 5, 24, 6, 25, 3], [1, 26, 27, 28, 7, 29, 30, 31, 2, 1, 32, 33, 34, 35, 36, 1, 37, 38, 2, 39, 4, 40, 3], [1, 41, 42, 1, 43, 44, 7, 45, 46, 47, 48, 49, 50]]\n",
            "input: [[2, 8, 9, 10, 11, 2, 1, 12, 4, 13], [14, 15, 16, 1, 17, 2, 1, 18, 19, 20, 5, 21, 6, 22, 23, 5, 24, 6, 25], [1, 26, 27, 28, 7, 29, 30, 31, 2, 1, 32, 33, 34, 35, 36, 1, 37, 38, 2, 39, 4, 40], [1, 41, 42, 1, 43, 44, 7, 45, 46, 47, 48, 49]]\n",
            "output: [[2, 8, 9, 10, 11, 2, 1, 12, 4, 13, 3], [14, 15, 16, 1, 17, 2, 1, 18, 19, 20, 5, 21, 6, 22, 23, 5, 24, 6, 25, 3], [1, 26, 27, 28, 7, 29, 30, 31, 2, 1, 32, 33, 34, 35, 36, 1, 37, 38, 2, 39, 4, 40, 3], [1, 41, 42, 1, 43, 44, 7, 45, 46, 47, 48, 49, 50]]\n",
            "AFTER padding: [[ 0  0  0  0  0  0  0  0  0  0  0  0  0  2  8  9 10 11  2  1 12  4 13]\n",
            " [ 0  0  0  0 14 15 16  1 17  2  1 18 19 20  5 21  6 22 23  5 24  6 25]\n",
            " [ 0  1 26 27 28  7 29 30 31  2  1 32 33 34 35 36  1 37 38  2 39  4 40]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  1 41 42  1 43 44  7 45 46 47 48 49]] shape:\n",
            " (4, 23)\n",
            "one hot vector y\n",
            " [[[1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 1. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 1. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 1. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 1.]]]\n",
            "(4, 23) (4, 23, 51)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GAbpPR5Bh0dq",
        "outputId": "250b70a1-e718-45d5-bca2-cc80f7323b8d"
      },
      "source": [
        "#making the model\n",
        "\n",
        "model=Sequential()\n",
        "model.add(Embedding(input_dim=vocab_len,output_dim=10))\n",
        "print(model.input_shape)\n",
        "print(model.output_shape)\n",
        "\n",
        "model.add(SimpleRNN(50,return_sequences=True)) # out put of previous fold will be used in input\n",
        "print(model.output_shape)\n",
        "model.add(Dense(units=vocab_len,activation='softmax'))\n",
        "model.summary()\n",
        "model.compile(loss='categorical_crossentropy',optimizer='rmsprop',metrics=['accuracy'])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(None, None)\n",
            "(None, None, 10)\n",
            "(None, None, 50)\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, None, 10)          510       \n",
            "_________________________________________________________________\n",
            "simple_rnn (SimpleRNN)       (None, None, 50)          3050      \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, None, 51)          2601      \n",
            "=================================================================\n",
            "Total params: 6,161\n",
            "Trainable params: 6,161\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAfRiDG0ejCZ",
        "outputId": "f899fdfd-26e2-4f15-b10f-78dbb371df88"
      },
      "source": [
        "model.fit(X,Y,epochs=500,verbose=1)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "1/1 [==============================] - 4s 4s/step - loss: 3.9298 - accuracy: 0.0109\n",
            "Epoch 2/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 3.8963 - accuracy: 0.2717\n",
            "Epoch 3/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 3.8658 - accuracy: 0.3370\n",
            "Epoch 4/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 3.8285 - accuracy: 0.3478\n",
            "Epoch 5/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 3.7833 - accuracy: 0.3261\n",
            "Epoch 6/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 3.7401 - accuracy: 0.3370\n",
            "Epoch 7/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 3.7307 - accuracy: 0.2935\n",
            "Epoch 8/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 3.6694 - accuracy: 0.3261\n",
            "Epoch 9/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 3.6190 - accuracy: 0.3261\n",
            "Epoch 10/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 3.5754 - accuracy: 0.3261\n",
            "Epoch 11/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 3.5479 - accuracy: 0.3370\n",
            "Epoch 12/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 3.5433 - accuracy: 0.2935\n",
            "Epoch 13/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 3.5063 - accuracy: 0.3370\n",
            "Epoch 14/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 3.4391 - accuracy: 0.3370\n",
            "Epoch 15/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 3.4001 - accuracy: 0.3478\n",
            "Epoch 16/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 3.3637 - accuracy: 0.3478\n",
            "Epoch 17/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 3.3317 - accuracy: 0.3587\n",
            "Epoch 18/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 3.3115 - accuracy: 0.3261\n",
            "Epoch 19/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 3.3221 - accuracy: 0.3261\n",
            "Epoch 20/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 3.2595 - accuracy: 0.3152\n",
            "Epoch 21/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 3.2308 - accuracy: 0.3478\n",
            "Epoch 22/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 3.1865 - accuracy: 0.3370\n",
            "Epoch 23/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 3.1582 - accuracy: 0.3587\n",
            "Epoch 24/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 3.1292 - accuracy: 0.3478\n",
            "Epoch 25/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 3.1085 - accuracy: 0.3478\n",
            "Epoch 26/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 3.0871 - accuracy: 0.3152\n",
            "Epoch 27/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 3.0831 - accuracy: 0.3152\n",
            "Epoch 28/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 3.0484 - accuracy: 0.3152\n",
            "Epoch 29/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 3.0282 - accuracy: 0.3370\n",
            "Epoch 30/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 2.9877 - accuracy: 0.3370\n",
            "Epoch 31/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 2.9621 - accuracy: 0.3478\n",
            "Epoch 32/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 2.9363 - accuracy: 0.3478\n",
            "Epoch 33/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 2.9153 - accuracy: 0.3478\n",
            "Epoch 34/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 2.8959 - accuracy: 0.3478\n",
            "Epoch 35/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 2.8837 - accuracy: 0.3261\n",
            "Epoch 36/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 2.8709 - accuracy: 0.3478\n",
            "Epoch 37/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 2.8615 - accuracy: 0.3478\n",
            "Epoch 38/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 2.8308 - accuracy: 0.3587\n",
            "Epoch 39/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 2.8065 - accuracy: 0.3478\n",
            "Epoch 40/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 2.7788 - accuracy: 0.3696\n",
            "Epoch 41/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 2.7590 - accuracy: 0.3478\n",
            "Epoch 42/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 2.7389 - accuracy: 0.3696\n",
            "Epoch 43/500\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 2.7243 - accuracy: 0.3804\n",
            "Epoch 44/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 2.7087 - accuracy: 0.3696\n",
            "Epoch 45/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 2.6992 - accuracy: 0.3913\n",
            "Epoch 46/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 2.6810 - accuracy: 0.3804\n",
            "Epoch 47/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 2.6678 - accuracy: 0.4022\n",
            "Epoch 48/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 2.6422 - accuracy: 0.3913\n",
            "Epoch 49/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 2.6263 - accuracy: 0.4022\n",
            "Epoch 50/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 2.6034 - accuracy: 0.3913\n",
            "Epoch 51/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 2.5895 - accuracy: 0.4022\n",
            "Epoch 52/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 2.5702 - accuracy: 0.4130\n",
            "Epoch 53/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 2.5593 - accuracy: 0.4022\n",
            "Epoch 54/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 2.5408 - accuracy: 0.4130\n",
            "Epoch 55/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 2.5318 - accuracy: 0.4022\n",
            "Epoch 56/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 2.5108 - accuracy: 0.4130\n",
            "Epoch 57/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 2.5008 - accuracy: 0.3913\n",
            "Epoch 58/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 2.4778 - accuracy: 0.4130\n",
            "Epoch 59/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 2.4670 - accuracy: 0.4022\n",
            "Epoch 60/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 2.4451 - accuracy: 0.4130\n",
            "Epoch 61/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 2.4348 - accuracy: 0.4130\n",
            "Epoch 62/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 2.4144 - accuracy: 0.4130\n",
            "Epoch 63/500\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 2.4052 - accuracy: 0.4130\n",
            "Epoch 64/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 2.3850 - accuracy: 0.4457\n",
            "Epoch 65/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 2.3765 - accuracy: 0.4348\n",
            "Epoch 66/500\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 2.3555 - accuracy: 0.4457\n",
            "Epoch 67/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 2.3467 - accuracy: 0.4457\n",
            "Epoch 68/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 2.3251 - accuracy: 0.4565\n",
            "Epoch 69/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 2.3154 - accuracy: 0.4565\n",
            "Epoch 70/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 2.2943 - accuracy: 0.4565\n",
            "Epoch 71/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 2.2843 - accuracy: 0.4783\n",
            "Epoch 72/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 2.2641 - accuracy: 0.4674\n",
            "Epoch 73/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 2.2542 - accuracy: 0.4891\n",
            "Epoch 74/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 2.2343 - accuracy: 0.4674\n",
            "Epoch 75/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 2.2243 - accuracy: 0.5109\n",
            "Epoch 76/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 2.2041 - accuracy: 0.4891\n",
            "Epoch 77/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 2.1934 - accuracy: 0.5217\n",
            "Epoch 78/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 2.1735 - accuracy: 0.5217\n",
            "Epoch 79/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 2.1625 - accuracy: 0.5217\n",
            "Epoch 80/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 2.1438 - accuracy: 0.5326\n",
            "Epoch 81/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 2.1325 - accuracy: 0.5326\n",
            "Epoch 82/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 2.1137 - accuracy: 0.5326\n",
            "Epoch 83/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 2.0992 - accuracy: 0.5435\n",
            "Epoch 84/500\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 2.0795 - accuracy: 0.5326\n",
            "Epoch 85/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 2.0649 - accuracy: 0.5652\n",
            "Epoch 86/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 2.0472 - accuracy: 0.5543\n",
            "Epoch 87/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 2.0349 - accuracy: 0.5761\n",
            "Epoch 88/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 2.0192 - accuracy: 0.5543\n",
            "Epoch 89/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 2.0074 - accuracy: 0.5761\n",
            "Epoch 90/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.9882 - accuracy: 0.5543\n",
            "Epoch 91/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.9736 - accuracy: 0.5761\n",
            "Epoch 92/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.9547 - accuracy: 0.5652\n",
            "Epoch 93/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.9416 - accuracy: 0.5870\n",
            "Epoch 94/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.9253 - accuracy: 0.5870\n",
            "Epoch 95/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.9124 - accuracy: 0.6087\n",
            "Epoch 96/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 1.8950 - accuracy: 0.6196\n",
            "Epoch 97/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.8786 - accuracy: 0.6087\n",
            "Epoch 98/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 1.8602 - accuracy: 0.6087\n",
            "Epoch 99/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 1.8446 - accuracy: 0.6087\n",
            "Epoch 100/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.8280 - accuracy: 0.6087\n",
            "Epoch 101/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.8140 - accuracy: 0.6304\n",
            "Epoch 102/500\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 1.7992 - accuracy: 0.6087\n",
            "Epoch 103/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 1.7867 - accuracy: 0.6413\n",
            "Epoch 104/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 1.7710 - accuracy: 0.6413\n",
            "Epoch 105/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 1.7576 - accuracy: 0.6630\n",
            "Epoch 106/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.7394 - accuracy: 0.6522\n",
            "Epoch 107/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.7253 - accuracy: 0.6848\n",
            "Epoch 108/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.7081 - accuracy: 0.6848\n",
            "Epoch 109/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.6946 - accuracy: 0.6957\n",
            "Epoch 110/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.6783 - accuracy: 0.6957\n",
            "Epoch 111/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.6638 - accuracy: 0.6957\n",
            "Epoch 112/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.6471 - accuracy: 0.6957\n",
            "Epoch 113/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 1.6316 - accuracy: 0.6957\n",
            "Epoch 114/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.6152 - accuracy: 0.7174\n",
            "Epoch 115/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.6008 - accuracy: 0.7174\n",
            "Epoch 116/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 1.5858 - accuracy: 0.7283\n",
            "Epoch 117/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 1.5730 - accuracy: 0.7283\n",
            "Epoch 118/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 1.5596 - accuracy: 0.7283\n",
            "Epoch 119/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.5485 - accuracy: 0.7283\n",
            "Epoch 120/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 1.5336 - accuracy: 0.7283\n",
            "Epoch 121/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.5201 - accuracy: 0.7283\n",
            "Epoch 122/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 1.5003 - accuracy: 0.7283\n",
            "Epoch 123/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 1.4851 - accuracy: 0.7283\n",
            "Epoch 124/500\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 1.4681 - accuracy: 0.7391\n",
            "Epoch 125/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 1.4550 - accuracy: 0.7283\n",
            "Epoch 126/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 1.4410 - accuracy: 0.7391\n",
            "Epoch 127/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.4303 - accuracy: 0.7283\n",
            "Epoch 128/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 1.4177 - accuracy: 0.7391\n",
            "Epoch 129/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.4040 - accuracy: 0.7283\n",
            "Epoch 130/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.3890 - accuracy: 0.7391\n",
            "Epoch 131/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 1.3748 - accuracy: 0.7283\n",
            "Epoch 132/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 1.3595 - accuracy: 0.7391\n",
            "Epoch 133/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 1.3460 - accuracy: 0.7609\n",
            "Epoch 134/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.3311 - accuracy: 0.7391\n",
            "Epoch 135/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 1.3181 - accuracy: 0.7609\n",
            "Epoch 136/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 1.3041 - accuracy: 0.7500\n",
            "Epoch 137/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 1.2924 - accuracy: 0.7609\n",
            "Epoch 138/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.2805 - accuracy: 0.7391\n",
            "Epoch 139/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 1.2727 - accuracy: 0.7500\n",
            "Epoch 140/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.2571 - accuracy: 0.7391\n",
            "Epoch 141/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.2465 - accuracy: 0.7609\n",
            "Epoch 142/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 1.2264 - accuracy: 0.7500\n",
            "Epoch 143/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 1.2127 - accuracy: 0.7609\n",
            "Epoch 144/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 1.1978 - accuracy: 0.7717\n",
            "Epoch 145/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 1.1856 - accuracy: 0.7717\n",
            "Epoch 146/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.1742 - accuracy: 0.7717\n",
            "Epoch 147/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 1.1654 - accuracy: 0.7717\n",
            "Epoch 148/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 1.1561 - accuracy: 0.7826\n",
            "Epoch 149/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.1475 - accuracy: 0.7935\n",
            "Epoch 150/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.1313 - accuracy: 0.7935\n",
            "Epoch 151/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.1173 - accuracy: 0.8043\n",
            "Epoch 152/500\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 1.1007 - accuracy: 0.7935\n",
            "Epoch 153/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 1.0876 - accuracy: 0.8152\n",
            "Epoch 154/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.0748 - accuracy: 0.7935\n",
            "Epoch 155/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.0648 - accuracy: 0.8043\n",
            "Epoch 156/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.0554 - accuracy: 0.8152\n",
            "Epoch 157/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.0510 - accuracy: 0.7935\n",
            "Epoch 158/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.0369 - accuracy: 0.8152\n",
            "Epoch 159/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.0258 - accuracy: 0.8152\n",
            "Epoch 160/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.0071 - accuracy: 0.8370\n",
            "Epoch 161/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.9941 - accuracy: 0.8478\n",
            "Epoch 162/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.9812 - accuracy: 0.8370\n",
            "Epoch 163/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.9713 - accuracy: 0.8478\n",
            "Epoch 164/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.9626 - accuracy: 0.8370\n",
            "Epoch 165/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.9585 - accuracy: 0.8696\n",
            "Epoch 166/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.9478 - accuracy: 0.8478\n",
            "Epoch 167/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.9397 - accuracy: 0.8696\n",
            "Epoch 168/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.9194 - accuracy: 0.8587\n",
            "Epoch 169/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.9060 - accuracy: 0.8696\n",
            "Epoch 170/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.8921 - accuracy: 0.8587\n",
            "Epoch 171/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.8811 - accuracy: 0.8696\n",
            "Epoch 172/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.8703 - accuracy: 0.8696\n",
            "Epoch 173/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.8624 - accuracy: 0.8804\n",
            "Epoch 174/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.8551 - accuracy: 0.8696\n",
            "Epoch 175/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.8521 - accuracy: 0.8804\n",
            "Epoch 176/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.8387 - accuracy: 0.8696\n",
            "Epoch 177/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.8278 - accuracy: 0.8804\n",
            "Epoch 178/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.8107 - accuracy: 0.8804\n",
            "Epoch 179/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.7991 - accuracy: 0.8913\n",
            "Epoch 180/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.7875 - accuracy: 0.8804\n",
            "Epoch 181/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.7792 - accuracy: 0.8913\n",
            "Epoch 182/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.7718 - accuracy: 0.8696\n",
            "Epoch 183/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.7711 - accuracy: 0.8913\n",
            "Epoch 184/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.7601 - accuracy: 0.8587\n",
            "Epoch 185/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.7548 - accuracy: 0.8913\n",
            "Epoch 186/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.7322 - accuracy: 0.8804\n",
            "Epoch 187/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.7202 - accuracy: 0.9022\n",
            "Epoch 188/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.7079 - accuracy: 0.9022\n",
            "Epoch 189/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.6985 - accuracy: 0.9130\n",
            "Epoch 190/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.6894 - accuracy: 0.9022\n",
            "Epoch 191/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.6836 - accuracy: 0.9130\n",
            "Epoch 192/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.6789 - accuracy: 0.9022\n",
            "Epoch 193/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.6796 - accuracy: 0.9239\n",
            "Epoch 194/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.6648 - accuracy: 0.9130\n",
            "Epoch 195/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.6554 - accuracy: 0.9348\n",
            "Epoch 196/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6397 - accuracy: 0.9130\n",
            "Epoch 197/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6302 - accuracy: 0.9348\n",
            "Epoch 198/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.6202 - accuracy: 0.9239\n",
            "Epoch 199/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6132 - accuracy: 0.9348\n",
            "Epoch 200/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.6064 - accuracy: 0.9239\n",
            "Epoch 201/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.6060 - accuracy: 0.9348\n",
            "Epoch 202/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.5986 - accuracy: 0.9239\n",
            "Epoch 203/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.5991 - accuracy: 0.9348\n",
            "Epoch 204/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.5761 - accuracy: 0.9239\n",
            "Epoch 205/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.5661 - accuracy: 0.9348\n",
            "Epoch 206/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.5549 - accuracy: 0.9239\n",
            "Epoch 207/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.5473 - accuracy: 0.9348\n",
            "Epoch 208/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.5398 - accuracy: 0.9239\n",
            "Epoch 209/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.5347 - accuracy: 0.9348\n",
            "Epoch 210/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.5289 - accuracy: 0.9239\n",
            "Epoch 211/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.5281 - accuracy: 0.9348\n",
            "Epoch 212/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.5177 - accuracy: 0.9239\n",
            "Epoch 213/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.5148 - accuracy: 0.9348\n",
            "Epoch 214/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.5007 - accuracy: 0.9239\n",
            "Epoch 215/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.4949 - accuracy: 0.9348\n",
            "Epoch 216/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.4855 - accuracy: 0.9348\n",
            "Epoch 217/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4804 - accuracy: 0.9348\n",
            "Epoch 218/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.4736 - accuracy: 0.9348\n",
            "Epoch 219/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.4724 - accuracy: 0.9348\n",
            "Epoch 220/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.4643 - accuracy: 0.9348\n",
            "Epoch 221/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.4638 - accuracy: 0.9457\n",
            "Epoch 222/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.4481 - accuracy: 0.9348\n",
            "Epoch 223/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.4416 - accuracy: 0.9457\n",
            "Epoch 224/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.4323 - accuracy: 0.9457\n",
            "Epoch 225/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4269 - accuracy: 0.9457\n",
            "Epoch 226/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.4206 - accuracy: 0.9457\n",
            "Epoch 227/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.4172 - accuracy: 0.9457\n",
            "Epoch 228/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.4116 - accuracy: 0.9457\n",
            "Epoch 229/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4108 - accuracy: 0.9457\n",
            "Epoch 230/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4016 - accuracy: 0.9457\n",
            "Epoch 231/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.3990 - accuracy: 0.9457\n",
            "Epoch 232/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.3882 - accuracy: 0.9457\n",
            "Epoch 233/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.3838 - accuracy: 0.9565\n",
            "Epoch 234/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.3763 - accuracy: 0.9565\n",
            "Epoch 235/500\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.3730 - accuracy: 0.9565\n",
            "Epoch 236/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.3677 - accuracy: 0.9565\n",
            "Epoch 237/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.3679 - accuracy: 0.9674\n",
            "Epoch 238/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.3596 - accuracy: 0.9565\n",
            "Epoch 239/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.3577 - accuracy: 0.9674\n",
            "Epoch 240/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.3463 - accuracy: 0.9565\n",
            "Epoch 241/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.3416 - accuracy: 0.9674\n",
            "Epoch 242/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.3350 - accuracy: 0.9565\n",
            "Epoch 243/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.3316 - accuracy: 0.9674\n",
            "Epoch 244/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.3267 - accuracy: 0.9674\n",
            "Epoch 245/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.3250 - accuracy: 0.9565\n",
            "Epoch 246/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.3193 - accuracy: 0.9674\n",
            "Epoch 247/500\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.3178 - accuracy: 0.9674\n",
            "Epoch 248/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.3099 - accuracy: 0.9674\n",
            "Epoch 249/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.3070 - accuracy: 0.9674\n",
            "Epoch 250/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.3000 - accuracy: 0.9674\n",
            "Epoch 251/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.2973 - accuracy: 0.9674\n",
            "Epoch 252/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.2923 - accuracy: 0.9674\n",
            "Epoch 253/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.2919 - accuracy: 0.9674\n",
            "Epoch 254/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.2862 - accuracy: 0.9565\n",
            "Epoch 255/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.2855 - accuracy: 0.9674\n",
            "Epoch 256/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.2766 - accuracy: 0.9674\n",
            "Epoch 257/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.2729 - accuracy: 0.9674\n",
            "Epoch 258/500\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.2669 - accuracy: 0.9674\n",
            "Epoch 259/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.2639 - accuracy: 0.9674\n",
            "Epoch 260/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.2596 - accuracy: 0.9674\n",
            "Epoch 261/500\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.2578 - accuracy: 0.9674\n",
            "Epoch 262/500\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.2539 - accuracy: 0.9674\n",
            "Epoch 263/500\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.2532 - accuracy: 0.9674\n",
            "Epoch 264/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.2478 - accuracy: 0.9674\n",
            "Epoch 265/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.2462 - accuracy: 0.9674\n",
            "Epoch 266/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.2401 - accuracy: 0.9674\n",
            "Epoch 267/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.2381 - accuracy: 0.9674\n",
            "Epoch 268/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.2333 - accuracy: 0.9674\n",
            "Epoch 269/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.2325 - accuracy: 0.9674\n",
            "Epoch 270/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.2276 - accuracy: 0.9674\n",
            "Epoch 271/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.2264 - accuracy: 0.9674\n",
            "Epoch 272/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.2208 - accuracy: 0.9674\n",
            "Epoch 273/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.2189 - accuracy: 0.9674\n",
            "Epoch 274/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.2144 - accuracy: 0.9674\n",
            "Epoch 275/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.2127 - accuracy: 0.9674\n",
            "Epoch 276/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.2089 - accuracy: 0.9674\n",
            "Epoch 277/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.2075 - accuracy: 0.9674\n",
            "Epoch 278/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.2036 - accuracy: 0.9674\n",
            "Epoch 279/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.2024 - accuracy: 0.9674\n",
            "Epoch 280/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.1983 - accuracy: 0.9674\n",
            "Epoch 281/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.1970 - accuracy: 0.9674\n",
            "Epoch 282/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.1930 - accuracy: 0.9674\n",
            "Epoch 283/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.1918 - accuracy: 0.9674\n",
            "Epoch 284/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.1880 - accuracy: 0.9674\n",
            "Epoch 285/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.1871 - accuracy: 0.9674\n",
            "Epoch 286/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.1833 - accuracy: 0.9674\n",
            "Epoch 287/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.1826 - accuracy: 0.9674\n",
            "Epoch 288/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.1786 - accuracy: 0.9674\n",
            "Epoch 289/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.1776 - accuracy: 0.9674\n",
            "Epoch 290/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.1738 - accuracy: 0.9674\n",
            "Epoch 291/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.1727 - accuracy: 0.9674\n",
            "Epoch 292/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.1693 - accuracy: 0.9674\n",
            "Epoch 293/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.1682 - accuracy: 0.9674\n",
            "Epoch 294/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.1651 - accuracy: 0.9674\n",
            "Epoch 295/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.1642 - accuracy: 0.9674\n",
            "Epoch 296/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.1612 - accuracy: 0.9674\n",
            "Epoch 297/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.1607 - accuracy: 0.9674\n",
            "Epoch 298/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.1575 - accuracy: 0.9674\n",
            "Epoch 299/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.1567 - accuracy: 0.9674\n",
            "Epoch 300/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.1535 - accuracy: 0.9674\n",
            "Epoch 301/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.1525 - accuracy: 0.9674\n",
            "Epoch 302/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.1496 - accuracy: 0.9674\n",
            "Epoch 303/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.1487 - accuracy: 0.9674\n",
            "Epoch 304/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.1464 - accuracy: 0.9674\n",
            "Epoch 305/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.1465 - accuracy: 0.9674\n",
            "Epoch 306/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.1442 - accuracy: 0.9674\n",
            "Epoch 307/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.1453 - accuracy: 0.9674\n",
            "Epoch 308/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.1411 - accuracy: 0.9674\n",
            "Epoch 309/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.1401 - accuracy: 0.9674\n",
            "Epoch 310/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.1363 - accuracy: 0.9674\n",
            "Epoch 311/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.1350 - accuracy: 0.9674\n",
            "Epoch 312/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.1329 - accuracy: 0.9674\n",
            "Epoch 313/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.1321 - accuracy: 0.9674\n",
            "Epoch 314/500\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.1304 - accuracy: 0.9674\n",
            "Epoch 315/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.1302 - accuracy: 0.9674\n",
            "Epoch 316/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.1282 - accuracy: 0.9674\n",
            "Epoch 317/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.1284 - accuracy: 0.9674\n",
            "Epoch 318/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.1257 - accuracy: 0.9674\n",
            "Epoch 319/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.1257 - accuracy: 0.9674\n",
            "Epoch 320/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.1231 - accuracy: 0.9674\n",
            "Epoch 321/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.1230 - accuracy: 0.9674\n",
            "Epoch 322/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.1206 - accuracy: 0.9674\n",
            "Epoch 323/500\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.1205 - accuracy: 0.9674\n",
            "Epoch 324/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.1181 - accuracy: 0.9674\n",
            "Epoch 325/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.1178 - accuracy: 0.9674\n",
            "Epoch 326/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.1156 - accuracy: 0.9674\n",
            "Epoch 327/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.1152 - accuracy: 0.9674\n",
            "Epoch 328/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.1134 - accuracy: 0.9674\n",
            "Epoch 329/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.1134 - accuracy: 0.9674\n",
            "Epoch 330/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.1115 - accuracy: 0.9674\n",
            "Epoch 331/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.1118 - accuracy: 0.9674\n",
            "Epoch 332/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.1094 - accuracy: 0.9674\n",
            "Epoch 333/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.1092 - accuracy: 0.9674\n",
            "Epoch 334/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.1072 - accuracy: 0.9674\n",
            "Epoch 335/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.1068 - accuracy: 0.9674\n",
            "Epoch 336/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.1053 - accuracy: 0.9674\n",
            "Epoch 337/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.1052 - accuracy: 0.9674\n",
            "Epoch 338/500\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.1037 - accuracy: 0.9674\n",
            "Epoch 339/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.1042 - accuracy: 0.9674\n",
            "Epoch 340/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.1025 - accuracy: 0.9674\n",
            "Epoch 341/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.1035 - accuracy: 0.9674\n",
            "Epoch 342/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.1012 - accuracy: 0.9674\n",
            "Epoch 343/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.1016 - accuracy: 0.9674\n",
            "Epoch 344/500\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.0988 - accuracy: 0.9674\n",
            "Epoch 345/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.0979 - accuracy: 0.9674\n",
            "Epoch 346/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.0966 - accuracy: 0.9674\n",
            "Epoch 347/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.0960 - accuracy: 0.9674\n",
            "Epoch 348/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.0954 - accuracy: 0.9674\n",
            "Epoch 349/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.0962 - accuracy: 0.9674\n",
            "Epoch 350/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.0949 - accuracy: 0.9674\n",
            "Epoch 351/500\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.0962 - accuracy: 0.9674\n",
            "Epoch 352/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.0928 - accuracy: 0.9674\n",
            "Epoch 353/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.0924 - accuracy: 0.9674\n",
            "Epoch 354/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.0914 - accuracy: 0.9674\n",
            "Epoch 355/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.0911 - accuracy: 0.9674\n",
            "Epoch 356/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.0903 - accuracy: 0.9674\n",
            "Epoch 357/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.0907 - accuracy: 0.9674\n",
            "Epoch 358/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.0895 - accuracy: 0.9674\n",
            "Epoch 359/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.0903 - accuracy: 0.9674\n",
            "Epoch 360/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.0883 - accuracy: 0.9674\n",
            "Epoch 361/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.0885 - accuracy: 0.9674\n",
            "Epoch 362/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.0869 - accuracy: 0.9674\n",
            "Epoch 363/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.0866 - accuracy: 0.9674\n",
            "Epoch 364/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.0857 - accuracy: 0.9674\n",
            "Epoch 365/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.0858 - accuracy: 0.9674\n",
            "Epoch 366/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.0850 - accuracy: 0.9674\n",
            "Epoch 367/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.0857 - accuracy: 0.9674\n",
            "Epoch 368/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0841 - accuracy: 0.9674\n",
            "Epoch 369/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.0840 - accuracy: 0.9674\n",
            "Epoch 370/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.0829 - accuracy: 0.9674\n",
            "Epoch 371/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.0828 - accuracy: 0.9674\n",
            "Epoch 372/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.0821 - accuracy: 0.9674\n",
            "Epoch 373/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.0823 - accuracy: 0.9674\n",
            "Epoch 374/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.0815 - accuracy: 0.9674\n",
            "Epoch 375/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.0826 - accuracy: 0.9674\n",
            "Epoch 376/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.0813 - accuracy: 0.9674\n",
            "Epoch 377/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.0824 - accuracy: 0.9674\n",
            "Epoch 378/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.0800 - accuracy: 0.9674\n",
            "Epoch 379/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.0794 - accuracy: 0.9674\n",
            "Epoch 380/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.0785 - accuracy: 0.9674\n",
            "Epoch 381/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.0781 - accuracy: 0.9674\n",
            "Epoch 382/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.0778 - accuracy: 0.9674\n",
            "Epoch 383/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.0778 - accuracy: 0.9674\n",
            "Epoch 384/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.0781 - accuracy: 0.9674\n",
            "Epoch 385/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.0800 - accuracy: 0.9674\n",
            "Epoch 386/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.0769 - accuracy: 0.9674\n",
            "Epoch 387/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.0766 - accuracy: 0.9674\n",
            "Epoch 388/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.0761 - accuracy: 0.9674\n",
            "Epoch 389/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.0761 - accuracy: 0.9674\n",
            "Epoch 390/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.0757 - accuracy: 0.9674\n",
            "Epoch 391/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.0761 - accuracy: 0.9674\n",
            "Epoch 392/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.0752 - accuracy: 0.9674\n",
            "Epoch 393/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.0756 - accuracy: 0.9674\n",
            "Epoch 394/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.0746 - accuracy: 0.9674\n",
            "Epoch 395/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.0746 - accuracy: 0.9674\n",
            "Epoch 396/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.0740 - accuracy: 0.9674\n",
            "Epoch 397/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.0741 - accuracy: 0.9674\n",
            "Epoch 398/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.0736 - accuracy: 0.9674\n",
            "Epoch 399/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.0737 - accuracy: 0.9674\n",
            "Epoch 400/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.0731 - accuracy: 0.9674\n",
            "Epoch 401/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.0733 - accuracy: 0.9674\n",
            "Epoch 402/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0727 - accuracy: 0.9674\n",
            "Epoch 403/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.0727 - accuracy: 0.9674\n",
            "Epoch 404/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.0722 - accuracy: 0.9674\n",
            "Epoch 405/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.0723 - accuracy: 0.9674\n",
            "Epoch 406/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.0717 - accuracy: 0.9674\n",
            "Epoch 407/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.0719 - accuracy: 0.9674\n",
            "Epoch 408/500\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.0713 - accuracy: 0.9674\n",
            "Epoch 409/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.0716 - accuracy: 0.9674\n",
            "Epoch 410/500\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.0709 - accuracy: 0.9674\n",
            "Epoch 411/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.0713 - accuracy: 0.9674\n",
            "Epoch 412/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.0706 - accuracy: 0.9674\n",
            "Epoch 413/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.0709 - accuracy: 0.9674\n",
            "Epoch 414/500\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.0701 - accuracy: 0.9674\n",
            "Epoch 415/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.0700 - accuracy: 0.9674\n",
            "Epoch 416/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.0695 - accuracy: 0.9674\n",
            "Epoch 417/500\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.0693 - accuracy: 0.9674\n",
            "Epoch 418/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.0690 - accuracy: 0.9674\n",
            "Epoch 419/500\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.0689 - accuracy: 0.9674\n",
            "Epoch 420/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.0688 - accuracy: 0.9674\n",
            "Epoch 421/500\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.0694 - accuracy: 0.9674\n",
            "Epoch 422/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.0704 - accuracy: 0.9674\n",
            "Epoch 423/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.0720 - accuracy: 0.9674\n",
            "Epoch 424/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.0688 - accuracy: 0.9674\n",
            "Epoch 425/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.0683 - accuracy: 0.9674\n",
            "Epoch 426/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.0680 - accuracy: 0.9674\n",
            "Epoch 427/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.0678 - accuracy: 0.9674\n",
            "Epoch 428/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.0677 - accuracy: 0.9674\n",
            "Epoch 429/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.0676 - accuracy: 0.9674\n",
            "Epoch 430/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.0676 - accuracy: 0.9674\n",
            "Epoch 431/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.0679 - accuracy: 0.9674\n",
            "Epoch 432/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.0677 - accuracy: 0.9674\n",
            "Epoch 433/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.0681 - accuracy: 0.9674\n",
            "Epoch 434/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.0674 - accuracy: 0.9674\n",
            "Epoch 435/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.0673 - accuracy: 0.9674\n",
            "Epoch 436/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.0671 - accuracy: 0.9674\n",
            "Epoch 437/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.0672 - accuracy: 0.9674\n",
            "Epoch 438/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.0668 - accuracy: 0.9674\n",
            "Epoch 439/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.0667 - accuracy: 0.9674\n",
            "Epoch 440/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.0666 - accuracy: 0.9674\n",
            "Epoch 441/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.0668 - accuracy: 0.9674\n",
            "Epoch 442/500\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.0664 - accuracy: 0.9674\n",
            "Epoch 443/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.0664 - accuracy: 0.9674\n",
            "Epoch 444/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.0663 - accuracy: 0.9674\n",
            "Epoch 445/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.0666 - accuracy: 0.9674\n",
            "Epoch 446/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.0661 - accuracy: 0.9674\n",
            "Epoch 447/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.0659 - accuracy: 0.9674\n",
            "Epoch 448/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.0660 - accuracy: 0.9674\n",
            "Epoch 449/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.0665 - accuracy: 0.9674\n",
            "Epoch 450/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.0656 - accuracy: 0.9674\n",
            "Epoch 451/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.0655 - accuracy: 0.9674\n",
            "Epoch 452/500\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.0654 - accuracy: 0.9674\n",
            "Epoch 453/500\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.0655 - accuracy: 0.9674\n",
            "Epoch 454/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.0653 - accuracy: 0.9674\n",
            "Epoch 455/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.0654 - accuracy: 0.9674\n",
            "Epoch 456/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0652 - accuracy: 0.9674\n",
            "Epoch 457/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.0654 - accuracy: 0.9674\n",
            "Epoch 458/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.0650 - accuracy: 0.9674\n",
            "Epoch 459/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0649 - accuracy: 0.9674\n",
            "Epoch 460/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.0649 - accuracy: 0.9674\n",
            "Epoch 461/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.0654 - accuracy: 0.9674\n",
            "Epoch 462/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.0647 - accuracy: 0.9674\n",
            "Epoch 463/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.0646 - accuracy: 0.9674\n",
            "Epoch 464/500\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.0646 - accuracy: 0.9674\n",
            "Epoch 465/500\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.0646 - accuracy: 0.9674\n",
            "Epoch 466/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.0645 - accuracy: 0.9674\n",
            "Epoch 467/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.0647 - accuracy: 0.9674\n",
            "Epoch 468/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.0644 - accuracy: 0.9674\n",
            "Epoch 469/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.0645 - accuracy: 0.9674\n",
            "Epoch 470/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.0645 - accuracy: 0.9674\n",
            "Epoch 471/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.0650 - accuracy: 0.9674\n",
            "Epoch 472/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.0639 - accuracy: 0.9674\n",
            "Epoch 473/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.0637 - accuracy: 0.9674\n",
            "Epoch 474/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0637 - accuracy: 0.9674\n",
            "Epoch 475/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.0636 - accuracy: 0.9674\n",
            "Epoch 476/500\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.0636 - accuracy: 0.9674\n",
            "Epoch 477/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.0636 - accuracy: 0.9674\n",
            "Epoch 478/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.0636 - accuracy: 0.9674\n",
            "Epoch 479/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.0636 - accuracy: 0.9674\n",
            "Epoch 480/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.0636 - accuracy: 0.9674\n",
            "Epoch 481/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.0638 - accuracy: 0.9674\n",
            "Epoch 482/500\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.0648 - accuracy: 0.9674\n",
            "Epoch 483/500\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.0658 - accuracy: 0.9674\n",
            "Epoch 484/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.0634 - accuracy: 0.9674\n",
            "Epoch 485/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.0632 - accuracy: 0.9674\n",
            "Epoch 486/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.0631 - accuracy: 0.9674\n",
            "Epoch 487/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.0631 - accuracy: 0.9674\n",
            "Epoch 488/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.0630 - accuracy: 0.9674\n",
            "Epoch 489/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.0630 - accuracy: 0.9674\n",
            "Epoch 490/500\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.0630 - accuracy: 0.9674\n",
            "Epoch 491/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.0633 - accuracy: 0.9674\n",
            "Epoch 492/500\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.0637 - accuracy: 0.9674\n",
            "Epoch 493/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.0643 - accuracy: 0.9674\n",
            "Epoch 494/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.0640 - accuracy: 0.9674\n",
            "Epoch 495/500\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.0631 - accuracy: 0.9674\n",
            "Epoch 496/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.0633 - accuracy: 0.9674\n",
            "Epoch 497/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.0637 - accuracy: 0.9674\n",
            "Epoch 498/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.0630 - accuracy: 0.9674\n",
            "Epoch 499/500\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.0627 - accuracy: 0.9674\n",
            "Epoch 500/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.0626 - accuracy: 0.9674\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7faa004eda90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xeZdcROias5L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00470fb3-892e-4b6a-caee-cf67ba87ca63"
      },
      "source": [
        "# Sentance sampling generating all words without seed considering highest probability word as next word\n",
        "def sample_sequence(model_RNN, courpus_tokenizer, words, size_vocab):\n",
        "  enter = list()\n",
        "  text_input = ''\n",
        "  for i in range(words):\n",
        "    enter = courpus_tokenizer.texts_to_sequences([text_input])[0]\n",
        "    enter.insert(0,0)\n",
        "    enter = array(enter)\n",
        "    enter = numpy.reshape(enter, newshape=(1,-1))\n",
        "    print(\"i:\", i, \"Enter:\", enter, enter.shape)\n",
        "\n",
        "    if i==0:\n",
        "      probability = model_RNN.predict_proba(enter, verbose=0)\n",
        "      print(\"i:\", i, \"Probability:\", probability, type(probability), probability.shape)\n",
        "      y_predict = 0\n",
        "      while y_predict==0:\n",
        "        y_predict = numpy.random.choice(range(size_vocab), p = probability.ravel())\n",
        "      y_predict = numpy.reshape(numpy.array([y_predict]), newshape = (1,-1))\n",
        "      print(\"i:\", i, \"If Y_Predict:\", y_predict, y_predict.shape)\n",
        "    \n",
        "    else:\n",
        "      probability = model_RNN.predict_proba(enter, verbose=0)\n",
        "      print(\"i:\", i, \"Probability:\", probability, type(probability), probability.shape)\n",
        "      y_predict = numpy.append(y_predict,0)\n",
        "      y_predict = numpy.reshape(y_predict, newshape = (1,-1))\n",
        "\n",
        "      while y_predict[0,i]==0:\n",
        "        y_predict[0,i]=numpy.random.choice(range(size_vocab),p=probability[0,i].ravel())\n",
        "      print(\"i:\", i, \"Else Y_Predict:\", y_predict, y_predict.shape)\n",
        "\n",
        "    \n",
        "    exit = ''\n",
        "    for w, index in courpus_tokenizer.word_index.items():\n",
        "      if index == y_predict[0,i]:\n",
        "        exit = w\n",
        "        print(\"Index:\", i, \"Word:\", w)\n",
        "        break\n",
        "\n",
        "    \n",
        "    text_input = str(text_input) + str(exit) + ' '\n",
        "  return text_input\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(sample_sequence(model, tk, 8, vocab_len))\n",
        "    "
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "i: 0 Enter: [[0]] (1, 1)\n",
            "i: 0 Probability: [[[7.46949375e-01 2.46793196e-01 1.83577335e-03 1.88097212e-04\n",
            "   4.71277017e-05 3.81900812e-04 1.91070603e-06 2.09650784e-06\n",
            "   7.35414214e-04 1.49924992e-04 1.42769670e-04 1.34450411e-05\n",
            "   4.17966512e-05 3.41359519e-05 7.06000603e-04 1.92947791e-05\n",
            "   4.74016197e-05 5.33839884e-05 2.16858425e-05 5.09072561e-05\n",
            "   3.84136947e-05 4.94236156e-05 7.20680109e-05 1.75826626e-05\n",
            "   1.11210880e-04 1.13037415e-04 1.09643435e-04 4.92015934e-05\n",
            "   2.30149981e-05 5.09998790e-05 8.83653629e-05 6.67178319e-05\n",
            "   3.35102163e-06 1.27836154e-06 1.49351672e-05 1.40283109e-05\n",
            "   8.95757112e-05 1.54491372e-05 1.04128580e-04 1.66033060e-04\n",
            "   1.47720042e-04 1.81505911e-05 6.44067995e-06 3.05070189e-05\n",
            "   5.64049005e-05 1.59723140e-06 4.23822576e-06 1.75711302e-05\n",
            "   1.56284441e-05 2.06641329e-04 8.10355850e-05]]] <class 'numpy.ndarray'> (1, 1, 51)\n",
            "i: 0 If Y_Predict: [[1]] (1, 1)\n",
            "Index: 0 Word: the\n",
            "i: 1 Enter: [[0 1]] (1, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:425: UserWarning: `model.predict_proba()` is deprecated and will be removed after 2021-01-01. Please use `model.predict()` instead.\n",
            "  warnings.warn('`model.predict_proba()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "i: 1 Probability: [[[7.46949434e-01 2.46793106e-01 1.83577160e-03 1.88097227e-04\n",
            "   4.71277053e-05 3.81900638e-04 1.91070603e-06 2.09650602e-06\n",
            "   7.35413807e-04 1.49924876e-04 1.42769539e-04 1.34450293e-05\n",
            "   4.17966148e-05 3.41359555e-05 7.06000312e-04 1.92947628e-05\n",
            "   4.74016233e-05 5.33839921e-05 2.16858243e-05 5.09072124e-05\n",
            "   3.84136656e-05 4.94235683e-05 7.20680182e-05 1.75826644e-05\n",
            "   1.11210888e-04 1.13037429e-04 1.09643348e-04 4.92015970e-05\n",
            "   2.30149781e-05 5.09998317e-05 8.83653702e-05 6.67178392e-05\n",
            "   3.35102186e-06 1.27836165e-06 1.49351690e-05 1.40283119e-05\n",
            "   8.95756384e-05 1.54491081e-05 1.04128587e-04 1.66032914e-04\n",
            "   1.47720057e-04 1.81505766e-05 6.44068086e-06 3.05069916e-05\n",
            "   5.64049042e-05 1.59723004e-06 4.23822621e-06 1.75711157e-05\n",
            "   1.56284441e-05 2.06641344e-04 8.10355923e-05]\n",
            "  [1.15814304e-03 2.31297945e-05 4.79320579e-06 9.19726954e-05\n",
            "   4.34800722e-06 7.80728783e-07 6.65805564e-05 4.89390914e-06\n",
            "   2.54268853e-05 1.21166144e-04 1.30465355e-06 2.11653605e-05\n",
            "   2.59782246e-04 1.58149014e-05 7.26251601e-05 3.46075045e-04\n",
            "   5.48814160e-06 1.37472176e-04 1.58094670e-04 7.71803798e-07\n",
            "   5.70955171e-06 8.59276915e-05 2.28467655e-07 7.71651539e-05\n",
            "   1.31000343e-04 3.49796778e-07 9.95303631e-01 4.74682838e-06\n",
            "   6.30278882e-05 5.89857245e-06 1.12038128e-06 3.78145123e-05\n",
            "   3.42089101e-04 1.31514184e-06 1.34916229e-08 3.48309150e-05\n",
            "   1.04459116e-06 3.45064072e-05 1.86913644e-06 7.41871645e-06\n",
            "   1.93782453e-05 1.11293280e-03 4.06533809e-06 2.58139062e-05\n",
            "   7.76692985e-08 4.20006607e-08 3.56084769e-07 2.12550901e-07\n",
            "   2.84282305e-06 1.51760878e-06 1.73211607e-04]]] <class 'numpy.ndarray'> (1, 2, 51)\n",
            "i: 1 Else Y_Predict: [[ 1 26]] (1, 2)\n",
            "Index: 1 Word: game\n",
            "i: 2 Enter: [[ 0  1 26]] (1, 3)\n",
            "i: 2 Probability: [[[7.46949434e-01 2.46793106e-01 1.83577160e-03 1.88097227e-04\n",
            "   4.71277053e-05 3.81900638e-04 1.91070603e-06 2.09650602e-06\n",
            "   7.35413807e-04 1.49924876e-04 1.42769539e-04 1.34450293e-05\n",
            "   4.17966148e-05 3.41359555e-05 7.06000312e-04 1.92947628e-05\n",
            "   4.74016233e-05 5.33839921e-05 2.16858243e-05 5.09072124e-05\n",
            "   3.84136656e-05 4.94235683e-05 7.20680182e-05 1.75826644e-05\n",
            "   1.11210888e-04 1.13037429e-04 1.09643348e-04 4.92015970e-05\n",
            "   2.30149781e-05 5.09998317e-05 8.83653702e-05 6.67178392e-05\n",
            "   3.35102186e-06 1.27836165e-06 1.49351690e-05 1.40283119e-05\n",
            "   8.95756384e-05 1.54491081e-05 1.04128587e-04 1.66032914e-04\n",
            "   1.47720057e-04 1.81505766e-05 6.44068086e-06 3.05069916e-05\n",
            "   5.64049042e-05 1.59723004e-06 4.23822621e-06 1.75711157e-05\n",
            "   1.56284441e-05 2.06641344e-04 8.10355923e-05]\n",
            "  [1.15814304e-03 2.31297945e-05 4.79320579e-06 9.19726954e-05\n",
            "   4.34800722e-06 7.80728783e-07 6.65805564e-05 4.89390914e-06\n",
            "   2.54268853e-05 1.21166144e-04 1.30465355e-06 2.11653605e-05\n",
            "   2.59782246e-04 1.58149014e-05 7.26251601e-05 3.46075045e-04\n",
            "   5.48814160e-06 1.37472176e-04 1.58094670e-04 7.71803798e-07\n",
            "   5.70955171e-06 8.59276915e-05 2.28467655e-07 7.71651539e-05\n",
            "   1.31000343e-04 3.49796778e-07 9.95303631e-01 4.74682838e-06\n",
            "   6.30278882e-05 5.89857245e-06 1.12038128e-06 3.78145123e-05\n",
            "   3.42089101e-04 1.31514184e-06 1.34916229e-08 3.48309150e-05\n",
            "   1.04459116e-06 3.45064072e-05 1.86913644e-06 7.41871645e-06\n",
            "   1.93782453e-05 1.11293280e-03 4.06533809e-06 2.58139062e-05\n",
            "   7.76692985e-08 4.20006607e-08 3.56084769e-07 2.12550901e-07\n",
            "   2.84282305e-06 1.51760878e-06 1.73211607e-04]\n",
            "  [2.40841086e-06 6.99716338e-08 3.30872081e-06 1.13340548e-06\n",
            "   4.32171328e-05 8.80145581e-06 1.54161520e-04 6.49309732e-06\n",
            "   1.12835914e-06 7.69849237e-07 8.47356787e-05 3.67667008e-06\n",
            "   1.50884416e-05 4.80783967e-07 3.53971927e-06 1.96342153e-05\n",
            "   2.97365732e-05 4.95144377e-07 8.75559363e-06 7.53171189e-05\n",
            "   7.01860330e-08 2.83522212e-07 1.06464347e-06 2.20338112e-08\n",
            "   4.55727047e-07 5.01941322e-05 1.10291580e-06 9.98720944e-01\n",
            "   9.58320470e-07 3.99690066e-07 6.67124141e-06 9.11050861e-08\n",
            "   8.28185075e-05 2.67268369e-05 8.72307240e-08 9.31407840e-08\n",
            "   9.60272246e-06 4.74259537e-07 8.32897331e-06 1.55092512e-05\n",
            "   6.92256663e-06 1.24879698e-05 4.82919451e-04 2.96014349e-07\n",
            "   4.71713720e-05 5.07895857e-05 1.12270840e-07 1.34604727e-06\n",
            "   1.87145815e-07 8.92629851e-06 3.29169971e-08]]] <class 'numpy.ndarray'> (1, 3, 51)\n",
            "i: 2 Else Y_Predict: [[ 1 26 27]] (1, 3)\n",
            "Index: 2 Word: that\n",
            "i: 3 Enter: [[ 0  1 26 27]] (1, 4)\n",
            "i: 3 Probability: [[[7.46949434e-01 2.46793106e-01 1.83577160e-03 1.88097227e-04\n",
            "   4.71277053e-05 3.81900638e-04 1.91070603e-06 2.09650602e-06\n",
            "   7.35413807e-04 1.49924876e-04 1.42769539e-04 1.34450293e-05\n",
            "   4.17966148e-05 3.41359555e-05 7.06000312e-04 1.92947628e-05\n",
            "   4.74016233e-05 5.33839921e-05 2.16858243e-05 5.09072124e-05\n",
            "   3.84136656e-05 4.94235683e-05 7.20680182e-05 1.75826644e-05\n",
            "   1.11210888e-04 1.13037429e-04 1.09643348e-04 4.92015970e-05\n",
            "   2.30149781e-05 5.09998317e-05 8.83653702e-05 6.67178392e-05\n",
            "   3.35102186e-06 1.27836165e-06 1.49351690e-05 1.40283119e-05\n",
            "   8.95756384e-05 1.54491081e-05 1.04128587e-04 1.66032914e-04\n",
            "   1.47720057e-04 1.81505766e-05 6.44068086e-06 3.05069916e-05\n",
            "   5.64049042e-05 1.59723004e-06 4.23822621e-06 1.75711157e-05\n",
            "   1.56284441e-05 2.06641344e-04 8.10355923e-05]\n",
            "  [1.15814304e-03 2.31297945e-05 4.79320579e-06 9.19726954e-05\n",
            "   4.34800722e-06 7.80728783e-07 6.65805564e-05 4.89390914e-06\n",
            "   2.54268853e-05 1.21166144e-04 1.30465355e-06 2.11653605e-05\n",
            "   2.59782246e-04 1.58149014e-05 7.26251601e-05 3.46075045e-04\n",
            "   5.48814160e-06 1.37472176e-04 1.58094670e-04 7.71803798e-07\n",
            "   5.70955171e-06 8.59276915e-05 2.28467655e-07 7.71651539e-05\n",
            "   1.31000343e-04 3.49796778e-07 9.95303631e-01 4.74682838e-06\n",
            "   6.30278882e-05 5.89857245e-06 1.12038128e-06 3.78145123e-05\n",
            "   3.42089101e-04 1.31514184e-06 1.34916229e-08 3.48309150e-05\n",
            "   1.04459116e-06 3.45064072e-05 1.86913644e-06 7.41871645e-06\n",
            "   1.93782453e-05 1.11293280e-03 4.06533809e-06 2.58139062e-05\n",
            "   7.76692985e-08 4.20006607e-08 3.56084769e-07 2.12550901e-07\n",
            "   2.84282305e-06 1.51760878e-06 1.73211607e-04]\n",
            "  [2.40841086e-06 6.99716338e-08 3.30872081e-06 1.13340548e-06\n",
            "   4.32171328e-05 8.80145581e-06 1.54161520e-04 6.49309732e-06\n",
            "   1.12835914e-06 7.69849237e-07 8.47356787e-05 3.67667008e-06\n",
            "   1.50884416e-05 4.80783967e-07 3.53971927e-06 1.96342153e-05\n",
            "   2.97365732e-05 4.95144377e-07 8.75559363e-06 7.53171189e-05\n",
            "   7.01860330e-08 2.83522212e-07 1.06464347e-06 2.20338112e-08\n",
            "   4.55727047e-07 5.01941322e-05 1.10291580e-06 9.98720944e-01\n",
            "   9.58320470e-07 3.99690066e-07 6.67124141e-06 9.11050861e-08\n",
            "   8.28185075e-05 2.67268369e-05 8.72307240e-08 9.31407840e-08\n",
            "   9.60272246e-06 4.74259537e-07 8.32897331e-06 1.55092512e-05\n",
            "   6.92256663e-06 1.24879698e-05 4.82919451e-04 2.96014349e-07\n",
            "   4.71713720e-05 5.07895857e-05 1.12270840e-07 1.34604727e-06\n",
            "   1.87145815e-07 8.92629851e-06 3.29169971e-08]\n",
            "  [1.70432642e-07 2.08761019e-04 2.25213785e-06 1.19964552e-05\n",
            "   3.46390843e-05 4.87494045e-10 5.85617563e-08 2.64177779e-05\n",
            "   3.13463011e-06 1.90825745e-06 5.09404401e-07 5.40846704e-05\n",
            "   8.51510418e-10 1.42344172e-04 9.15360124e-06 7.67936399e-06\n",
            "   2.96492126e-05 4.31939134e-06 1.86437781e-08 1.59102592e-06\n",
            "   8.18535955e-06 5.90161129e-08 3.89059387e-05 4.40502248e-07\n",
            "   3.27714133e-06 6.13578595e-06 3.04447872e-06 5.27338635e-08\n",
            "   9.99061048e-01 2.36178857e-05 3.48341356e-09 4.74717090e-05\n",
            "   1.57209485e-07 1.28571250e-04 2.29352372e-06 3.21302139e-07\n",
            "   3.15365511e-08 8.64350113e-06 3.82154781e-07 6.66949518e-07\n",
            "   9.56450847e-08 1.87511323e-05 2.24688520e-05 1.18627042e-06\n",
            "   2.12594912e-07 4.11111785e-08 8.09656995e-05 4.89535834e-09\n",
            "   8.93302968e-07 5.15741938e-09 3.38680707e-06]]] <class 'numpy.ndarray'> (1, 4, 51)\n",
            "i: 3 Else Y_Predict: [[ 1 26 27 28]] (1, 4)\n",
            "Index: 3 Word: messi\n",
            "i: 4 Enter: [[ 0  1 26 27 28]] (1, 5)\n",
            "i: 4 Probability: [[[7.46949434e-01 2.46793106e-01 1.83577160e-03 1.88097227e-04\n",
            "   4.71277053e-05 3.81900638e-04 1.91070603e-06 2.09650602e-06\n",
            "   7.35413807e-04 1.49924876e-04 1.42769539e-04 1.34450293e-05\n",
            "   4.17966148e-05 3.41359555e-05 7.06000312e-04 1.92947628e-05\n",
            "   4.74016233e-05 5.33839921e-05 2.16858243e-05 5.09072124e-05\n",
            "   3.84136656e-05 4.94235683e-05 7.20680182e-05 1.75826644e-05\n",
            "   1.11210888e-04 1.13037429e-04 1.09643348e-04 4.92015970e-05\n",
            "   2.30149781e-05 5.09998317e-05 8.83653702e-05 6.67178392e-05\n",
            "   3.35102186e-06 1.27836165e-06 1.49351690e-05 1.40283119e-05\n",
            "   8.95756384e-05 1.54491081e-05 1.04128587e-04 1.66032914e-04\n",
            "   1.47720057e-04 1.81505766e-05 6.44068086e-06 3.05069916e-05\n",
            "   5.64049042e-05 1.59723004e-06 4.23822621e-06 1.75711157e-05\n",
            "   1.56284441e-05 2.06641344e-04 8.10355923e-05]\n",
            "  [1.15814304e-03 2.31297945e-05 4.79320579e-06 9.19726954e-05\n",
            "   4.34800722e-06 7.80728783e-07 6.65805564e-05 4.89390914e-06\n",
            "   2.54268853e-05 1.21166144e-04 1.30465355e-06 2.11653605e-05\n",
            "   2.59782246e-04 1.58149014e-05 7.26251601e-05 3.46075045e-04\n",
            "   5.48814160e-06 1.37472176e-04 1.58094670e-04 7.71803798e-07\n",
            "   5.70955171e-06 8.59276915e-05 2.28467655e-07 7.71651539e-05\n",
            "   1.31000343e-04 3.49796778e-07 9.95303631e-01 4.74682838e-06\n",
            "   6.30278882e-05 5.89857245e-06 1.12038128e-06 3.78145123e-05\n",
            "   3.42089101e-04 1.31514184e-06 1.34916229e-08 3.48309150e-05\n",
            "   1.04459116e-06 3.45064072e-05 1.86913644e-06 7.41871645e-06\n",
            "   1.93782453e-05 1.11293280e-03 4.06533809e-06 2.58139062e-05\n",
            "   7.76692985e-08 4.20006607e-08 3.56084769e-07 2.12550901e-07\n",
            "   2.84282305e-06 1.51760878e-06 1.73211607e-04]\n",
            "  [2.40841086e-06 6.99716338e-08 3.30872081e-06 1.13340548e-06\n",
            "   4.32171328e-05 8.80145581e-06 1.54161520e-04 6.49309732e-06\n",
            "   1.12835914e-06 7.69849237e-07 8.47356787e-05 3.67667008e-06\n",
            "   1.50884416e-05 4.80783967e-07 3.53971927e-06 1.96342153e-05\n",
            "   2.97365732e-05 4.95144377e-07 8.75559363e-06 7.53171189e-05\n",
            "   7.01860330e-08 2.83522212e-07 1.06464347e-06 2.20338112e-08\n",
            "   4.55727047e-07 5.01941322e-05 1.10291580e-06 9.98720944e-01\n",
            "   9.58320470e-07 3.99690066e-07 6.67124141e-06 9.11050861e-08\n",
            "   8.28185075e-05 2.67268369e-05 8.72307240e-08 9.31407840e-08\n",
            "   9.60272246e-06 4.74259537e-07 8.32897331e-06 1.55092512e-05\n",
            "   6.92256663e-06 1.24879698e-05 4.82919451e-04 2.96014349e-07\n",
            "   4.71713720e-05 5.07895857e-05 1.12270840e-07 1.34604727e-06\n",
            "   1.87145815e-07 8.92629851e-06 3.29169971e-08]\n",
            "  [1.70432642e-07 2.08761019e-04 2.25213785e-06 1.19964552e-05\n",
            "   3.46390843e-05 4.87494045e-10 5.85617563e-08 2.64177779e-05\n",
            "   3.13463011e-06 1.90825745e-06 5.09404401e-07 5.40846704e-05\n",
            "   8.51510418e-10 1.42344172e-04 9.15360124e-06 7.67936399e-06\n",
            "   2.96492126e-05 4.31939134e-06 1.86437781e-08 1.59102592e-06\n",
            "   8.18535955e-06 5.90161129e-08 3.89059387e-05 4.40502248e-07\n",
            "   3.27714133e-06 6.13578595e-06 3.04447872e-06 5.27338635e-08\n",
            "   9.99061048e-01 2.36178857e-05 3.48341356e-09 4.74717090e-05\n",
            "   1.57209485e-07 1.28571250e-04 2.29352372e-06 3.21302139e-07\n",
            "   3.15365511e-08 8.64350113e-06 3.82154781e-07 6.66949518e-07\n",
            "   9.56450847e-08 1.87511323e-05 2.24688520e-05 1.18627042e-06\n",
            "   2.12594912e-07 4.11111785e-08 8.09656995e-05 4.89535834e-09\n",
            "   8.93302968e-07 5.15741938e-09 3.38680707e-06]\n",
            "  [1.79430788e-06 1.89575405e-06 8.48776563e-06 5.15947613e-05\n",
            "   2.03765822e-08 1.24991857e-06 5.37377218e-07 9.97696579e-01\n",
            "   5.77237415e-07 2.18066202e-06 4.30356067e-06 1.82755923e-06\n",
            "   3.90777495e-06 3.31638912e-05 4.69078820e-08 3.85017134e-04\n",
            "   1.77271693e-04 1.92029074e-05 4.48127576e-05 3.50035378e-07\n",
            "   2.71868976e-05 2.98053173e-08 3.50446925e-07 5.95445053e-05\n",
            "   1.23358239e-08 3.19527248e-06 1.99740896e-07 3.19678584e-05\n",
            "   1.61881883e-06 3.40937459e-06 9.13711265e-05 3.70482738e-08\n",
            "   1.22874641e-04 1.51112927e-05 2.32727427e-04 3.13202909e-05\n",
            "   5.29063391e-06 5.96283244e-05 1.51931177e-04 3.58502189e-06\n",
            "   3.09158895e-05 5.32233389e-06 1.46277118e-04 1.09342844e-04\n",
            "   1.03058719e-05 8.13554871e-05 1.40057218e-05 3.19583924e-04\n",
            "   3.11768389e-08 6.67705262e-06 6.88135486e-08]]] <class 'numpy.ndarray'> (1, 5, 51)\n",
            "i: 4 Else Y_Predict: [[ 1 26 27 28  7]] (1, 5)\n",
            "Index: 4 Word: and\n",
            "i: 5 Enter: [[ 0  1 26 27 28  7]] (1, 6)\n",
            "i: 5 Probability: [[[7.46949434e-01 2.46793106e-01 1.83577160e-03 1.88097227e-04\n",
            "   4.71277053e-05 3.81900638e-04 1.91070603e-06 2.09650602e-06\n",
            "   7.35413807e-04 1.49924876e-04 1.42769539e-04 1.34450293e-05\n",
            "   4.17966148e-05 3.41359555e-05 7.06000312e-04 1.92947628e-05\n",
            "   4.74016233e-05 5.33839921e-05 2.16858243e-05 5.09072124e-05\n",
            "   3.84136656e-05 4.94235683e-05 7.20680182e-05 1.75826644e-05\n",
            "   1.11210888e-04 1.13037429e-04 1.09643348e-04 4.92015970e-05\n",
            "   2.30149781e-05 5.09998317e-05 8.83653702e-05 6.67178392e-05\n",
            "   3.35102186e-06 1.27836165e-06 1.49351690e-05 1.40283119e-05\n",
            "   8.95756384e-05 1.54491081e-05 1.04128587e-04 1.66032914e-04\n",
            "   1.47720057e-04 1.81505766e-05 6.44068086e-06 3.05069916e-05\n",
            "   5.64049042e-05 1.59723004e-06 4.23822621e-06 1.75711157e-05\n",
            "   1.56284441e-05 2.06641344e-04 8.10355923e-05]\n",
            "  [1.15814304e-03 2.31297945e-05 4.79320579e-06 9.19726954e-05\n",
            "   4.34800722e-06 7.80728783e-07 6.65805564e-05 4.89390914e-06\n",
            "   2.54268853e-05 1.21166144e-04 1.30465355e-06 2.11653605e-05\n",
            "   2.59782246e-04 1.58149014e-05 7.26251601e-05 3.46075045e-04\n",
            "   5.48814160e-06 1.37472176e-04 1.58094670e-04 7.71803798e-07\n",
            "   5.70955171e-06 8.59276915e-05 2.28467655e-07 7.71651539e-05\n",
            "   1.31000343e-04 3.49796778e-07 9.95303631e-01 4.74682838e-06\n",
            "   6.30278882e-05 5.89857245e-06 1.12038128e-06 3.78145123e-05\n",
            "   3.42089101e-04 1.31514184e-06 1.34916229e-08 3.48309150e-05\n",
            "   1.04459116e-06 3.45064072e-05 1.86913644e-06 7.41871645e-06\n",
            "   1.93782453e-05 1.11293280e-03 4.06533809e-06 2.58139062e-05\n",
            "   7.76692985e-08 4.20006607e-08 3.56084769e-07 2.12550901e-07\n",
            "   2.84282305e-06 1.51760878e-06 1.73211607e-04]\n",
            "  [2.40841086e-06 6.99716338e-08 3.30872081e-06 1.13340548e-06\n",
            "   4.32171328e-05 8.80145581e-06 1.54161520e-04 6.49309732e-06\n",
            "   1.12835914e-06 7.69849237e-07 8.47356787e-05 3.67667008e-06\n",
            "   1.50884416e-05 4.80783967e-07 3.53971927e-06 1.96342153e-05\n",
            "   2.97365732e-05 4.95144377e-07 8.75559363e-06 7.53171189e-05\n",
            "   7.01860330e-08 2.83522212e-07 1.06464347e-06 2.20338112e-08\n",
            "   4.55727047e-07 5.01941322e-05 1.10291580e-06 9.98720944e-01\n",
            "   9.58320470e-07 3.99690066e-07 6.67124141e-06 9.11050861e-08\n",
            "   8.28185075e-05 2.67268369e-05 8.72307240e-08 9.31407840e-08\n",
            "   9.60272246e-06 4.74259537e-07 8.32897331e-06 1.55092512e-05\n",
            "   6.92256663e-06 1.24879698e-05 4.82919451e-04 2.96014349e-07\n",
            "   4.71713720e-05 5.07895857e-05 1.12270840e-07 1.34604727e-06\n",
            "   1.87145815e-07 8.92629851e-06 3.29169971e-08]\n",
            "  [1.70432642e-07 2.08761019e-04 2.25213785e-06 1.19964552e-05\n",
            "   3.46390843e-05 4.87494045e-10 5.85617563e-08 2.64177779e-05\n",
            "   3.13463011e-06 1.90825745e-06 5.09404401e-07 5.40846704e-05\n",
            "   8.51510418e-10 1.42344172e-04 9.15360124e-06 7.67936399e-06\n",
            "   2.96492126e-05 4.31939134e-06 1.86437781e-08 1.59102592e-06\n",
            "   8.18535955e-06 5.90161129e-08 3.89059387e-05 4.40502248e-07\n",
            "   3.27714133e-06 6.13578595e-06 3.04447872e-06 5.27338635e-08\n",
            "   9.99061048e-01 2.36178857e-05 3.48341356e-09 4.74717090e-05\n",
            "   1.57209485e-07 1.28571250e-04 2.29352372e-06 3.21302139e-07\n",
            "   3.15365511e-08 8.64350113e-06 3.82154781e-07 6.66949518e-07\n",
            "   9.56450847e-08 1.87511323e-05 2.24688520e-05 1.18627042e-06\n",
            "   2.12594912e-07 4.11111785e-08 8.09656995e-05 4.89535834e-09\n",
            "   8.93302968e-07 5.15741938e-09 3.38680707e-06]\n",
            "  [1.79430788e-06 1.89575405e-06 8.48776563e-06 5.15947613e-05\n",
            "   2.03765822e-08 1.24991857e-06 5.37377218e-07 9.97696579e-01\n",
            "   5.77237415e-07 2.18066202e-06 4.30356067e-06 1.82755923e-06\n",
            "   3.90777495e-06 3.31638912e-05 4.69078820e-08 3.85017134e-04\n",
            "   1.77271693e-04 1.92029074e-05 4.48127576e-05 3.50035378e-07\n",
            "   2.71868976e-05 2.98053173e-08 3.50446925e-07 5.95445053e-05\n",
            "   1.23358239e-08 3.19527248e-06 1.99740896e-07 3.19678584e-05\n",
            "   1.61881883e-06 3.40937459e-06 9.13711265e-05 3.70482738e-08\n",
            "   1.22874641e-04 1.51112927e-05 2.32727427e-04 3.13202909e-05\n",
            "   5.29063391e-06 5.96283244e-05 1.51931177e-04 3.58502189e-06\n",
            "   3.09158895e-05 5.32233389e-06 1.46277118e-04 1.09342844e-04\n",
            "   1.03058719e-05 8.13554871e-05 1.40057218e-05 3.19583924e-04\n",
            "   3.11768389e-08 6.67705262e-06 6.88135486e-08]\n",
            "  [3.29312002e-07 4.29301319e-04 5.78979234e-06 2.41799917e-05\n",
            "   2.03164109e-06 6.38545998e-06 2.11715853e-10 2.24456630e-06\n",
            "   2.33376923e-07 1.47866686e-07 2.58488967e-06 3.35183358e-06\n",
            "   3.51871137e-07 1.34884630e-07 6.49217668e-07 3.20641966e-08\n",
            "   8.16493139e-06 1.06440479e-04 9.32391060e-07 3.14230374e-06\n",
            "   2.22892083e-07 6.19015964e-07 3.73214704e-08 1.06376795e-07\n",
            "   1.70237513e-07 7.21481968e-07 6.74310002e-07 3.73216693e-07\n",
            "   4.03544218e-05 9.98680770e-01 8.90696413e-07 3.13478668e-05\n",
            "   3.92616982e-07 2.02668398e-05 3.22160759e-06 2.37436601e-04\n",
            "   7.92016692e-07 2.02417763e-08 3.94648305e-05 1.70764524e-05\n",
            "   1.64967230e-06 4.14240139e-06 2.97207151e-07 4.74473964e-05\n",
            "   8.62934539e-05 7.49360879e-07 4.03892300e-05 1.52444363e-06\n",
            "   1.43383702e-04 3.00050687e-08 2.70292321e-06]]] <class 'numpy.ndarray'> (1, 6, 51)\n",
            "i: 5 Else Y_Predict: [[ 1 26 27 28  7 29]] (1, 6)\n",
            "Index: 5 Word: ronaldo\n",
            "i: 6 Enter: [[ 0  1 26 27 28  7 29]] (1, 7)\n",
            "i: 6 Probability: [[[7.46949434e-01 2.46793106e-01 1.83577160e-03 1.88097227e-04\n",
            "   4.71277053e-05 3.81900638e-04 1.91070603e-06 2.09650602e-06\n",
            "   7.35413807e-04 1.49924876e-04 1.42769539e-04 1.34450293e-05\n",
            "   4.17966148e-05 3.41359555e-05 7.06000312e-04 1.92947628e-05\n",
            "   4.74016233e-05 5.33839921e-05 2.16858243e-05 5.09072124e-05\n",
            "   3.84136656e-05 4.94235683e-05 7.20680182e-05 1.75826644e-05\n",
            "   1.11210888e-04 1.13037429e-04 1.09643348e-04 4.92015970e-05\n",
            "   2.30149781e-05 5.09998317e-05 8.83653702e-05 6.67178392e-05\n",
            "   3.35102186e-06 1.27836165e-06 1.49351690e-05 1.40283119e-05\n",
            "   8.95756384e-05 1.54491081e-05 1.04128587e-04 1.66032914e-04\n",
            "   1.47720057e-04 1.81505766e-05 6.44068086e-06 3.05069916e-05\n",
            "   5.64049042e-05 1.59723004e-06 4.23822621e-06 1.75711157e-05\n",
            "   1.56284441e-05 2.06641344e-04 8.10355923e-05]\n",
            "  [1.15814304e-03 2.31297945e-05 4.79320579e-06 9.19726954e-05\n",
            "   4.34800722e-06 7.80728783e-07 6.65805564e-05 4.89390914e-06\n",
            "   2.54268853e-05 1.21166144e-04 1.30465355e-06 2.11653605e-05\n",
            "   2.59782246e-04 1.58149014e-05 7.26251601e-05 3.46075045e-04\n",
            "   5.48814160e-06 1.37472176e-04 1.58094670e-04 7.71803798e-07\n",
            "   5.70955171e-06 8.59276915e-05 2.28467655e-07 7.71651539e-05\n",
            "   1.31000343e-04 3.49796778e-07 9.95303631e-01 4.74682838e-06\n",
            "   6.30278882e-05 5.89857245e-06 1.12038128e-06 3.78145123e-05\n",
            "   3.42089101e-04 1.31514184e-06 1.34916229e-08 3.48309150e-05\n",
            "   1.04459116e-06 3.45064072e-05 1.86913644e-06 7.41871645e-06\n",
            "   1.93782453e-05 1.11293280e-03 4.06533809e-06 2.58139062e-05\n",
            "   7.76692985e-08 4.20006607e-08 3.56084769e-07 2.12550901e-07\n",
            "   2.84282305e-06 1.51760878e-06 1.73211607e-04]\n",
            "  [2.40841086e-06 6.99716338e-08 3.30872081e-06 1.13340548e-06\n",
            "   4.32171328e-05 8.80145581e-06 1.54161520e-04 6.49309732e-06\n",
            "   1.12835914e-06 7.69849237e-07 8.47356787e-05 3.67667008e-06\n",
            "   1.50884416e-05 4.80783967e-07 3.53971927e-06 1.96342153e-05\n",
            "   2.97365732e-05 4.95144377e-07 8.75559363e-06 7.53171189e-05\n",
            "   7.01860330e-08 2.83522212e-07 1.06464347e-06 2.20338112e-08\n",
            "   4.55727047e-07 5.01941322e-05 1.10291580e-06 9.98720944e-01\n",
            "   9.58320470e-07 3.99690066e-07 6.67124141e-06 9.11050861e-08\n",
            "   8.28185075e-05 2.67268369e-05 8.72307240e-08 9.31407840e-08\n",
            "   9.60272246e-06 4.74259537e-07 8.32897331e-06 1.55092512e-05\n",
            "   6.92256663e-06 1.24879698e-05 4.82919451e-04 2.96014349e-07\n",
            "   4.71713720e-05 5.07895857e-05 1.12270840e-07 1.34604727e-06\n",
            "   1.87145815e-07 8.92629851e-06 3.29169971e-08]\n",
            "  [1.70432642e-07 2.08761019e-04 2.25213785e-06 1.19964552e-05\n",
            "   3.46390843e-05 4.87494045e-10 5.85617563e-08 2.64177779e-05\n",
            "   3.13463011e-06 1.90825745e-06 5.09404401e-07 5.40846704e-05\n",
            "   8.51510418e-10 1.42344172e-04 9.15360124e-06 7.67936399e-06\n",
            "   2.96492126e-05 4.31939134e-06 1.86437781e-08 1.59102592e-06\n",
            "   8.18535955e-06 5.90161129e-08 3.89059387e-05 4.40502248e-07\n",
            "   3.27714133e-06 6.13578595e-06 3.04447872e-06 5.27338635e-08\n",
            "   9.99061048e-01 2.36178857e-05 3.48341356e-09 4.74717090e-05\n",
            "   1.57209485e-07 1.28571250e-04 2.29352372e-06 3.21302139e-07\n",
            "   3.15365511e-08 8.64350113e-06 3.82154781e-07 6.66949518e-07\n",
            "   9.56450847e-08 1.87511323e-05 2.24688520e-05 1.18627042e-06\n",
            "   2.12594912e-07 4.11111785e-08 8.09656995e-05 4.89535834e-09\n",
            "   8.93302968e-07 5.15741938e-09 3.38680707e-06]\n",
            "  [1.79430788e-06 1.89575405e-06 8.48776563e-06 5.15947613e-05\n",
            "   2.03765822e-08 1.24991857e-06 5.37377218e-07 9.97696579e-01\n",
            "   5.77237415e-07 2.18066202e-06 4.30356067e-06 1.82755923e-06\n",
            "   3.90777495e-06 3.31638912e-05 4.69078820e-08 3.85017134e-04\n",
            "   1.77271693e-04 1.92029074e-05 4.48127576e-05 3.50035378e-07\n",
            "   2.71868976e-05 2.98053173e-08 3.50446925e-07 5.95445053e-05\n",
            "   1.23358239e-08 3.19527248e-06 1.99740896e-07 3.19678584e-05\n",
            "   1.61881883e-06 3.40937459e-06 9.13711265e-05 3.70482738e-08\n",
            "   1.22874641e-04 1.51112927e-05 2.32727427e-04 3.13202909e-05\n",
            "   5.29063391e-06 5.96283244e-05 1.51931177e-04 3.58502189e-06\n",
            "   3.09158895e-05 5.32233389e-06 1.46277118e-04 1.09342844e-04\n",
            "   1.03058719e-05 8.13554871e-05 1.40057218e-05 3.19583924e-04\n",
            "   3.11768389e-08 6.67705262e-06 6.88135486e-08]\n",
            "  [3.29312002e-07 4.29301319e-04 5.78979234e-06 2.41799917e-05\n",
            "   2.03164109e-06 6.38545998e-06 2.11715853e-10 2.24456630e-06\n",
            "   2.33376923e-07 1.47866686e-07 2.58488967e-06 3.35183358e-06\n",
            "   3.51871137e-07 1.34884630e-07 6.49217668e-07 3.20641966e-08\n",
            "   8.16493139e-06 1.06440479e-04 9.32391060e-07 3.14230374e-06\n",
            "   2.22892083e-07 6.19015964e-07 3.73214704e-08 1.06376795e-07\n",
            "   1.70237513e-07 7.21481968e-07 6.74310002e-07 3.73216693e-07\n",
            "   4.03544218e-05 9.98680770e-01 8.90696413e-07 3.13478668e-05\n",
            "   3.92616982e-07 2.02668398e-05 3.22160759e-06 2.37436601e-04\n",
            "   7.92016692e-07 2.02417763e-08 3.94648305e-05 1.70764524e-05\n",
            "   1.64967230e-06 4.14240139e-06 2.97207151e-07 4.74473964e-05\n",
            "   8.62934539e-05 7.49360879e-07 4.03892300e-05 1.52444363e-06\n",
            "   1.43383702e-04 3.00050687e-08 2.70292321e-06]\n",
            "  [1.62329127e-06 1.78288758e-06 3.10899341e-04 3.61953667e-08\n",
            "   3.46872099e-07 4.22301753e-07 1.60750074e-06 5.30781945e-05\n",
            "   2.33528183e-08 2.88955757e-08 4.97007875e-07 2.47941131e-07\n",
            "   4.62281423e-06 8.70880754e-08 6.81051731e-08 2.33953415e-07\n",
            "   3.45745725e-08 5.12696337e-04 2.86509094e-05 1.69725854e-06\n",
            "   3.48904700e-06 5.03527190e-07 1.00426210e-08 2.77801138e-09\n",
            "   8.50088853e-08 8.53719540e-10 5.99816417e-07 4.21576851e-06\n",
            "   2.64463051e-09 2.61827807e-07 9.98193324e-01 1.01609858e-05\n",
            "   1.20998586e-04 1.22804920e-06 7.13681366e-05 1.15297726e-05\n",
            "   5.84899273e-04 1.15008788e-05 4.20632887e-07 8.62455363e-06\n",
            "   7.23341827e-06 8.25909410e-07 1.49278856e-06 1.03887714e-05\n",
            "   6.83051303e-06 5.01645263e-06 4.65210022e-08 6.70991949e-06\n",
            "   1.88996478e-06 1.77936217e-05 4.54174121e-09]]] <class 'numpy.ndarray'> (1, 7, 51)\n",
            "i: 6 Else Y_Predict: [[ 1 26 27 28  7 29 30]] (1, 7)\n",
            "Index: 6 Word: have\n",
            "i: 7 Enter: [[ 0  1 26 27 28  7 29 30]] (1, 8)\n",
            "i: 7 Probability: [[[7.46949434e-01 2.46793106e-01 1.83577160e-03 1.88097227e-04\n",
            "   4.71277053e-05 3.81900638e-04 1.91070603e-06 2.09650602e-06\n",
            "   7.35413807e-04 1.49924876e-04 1.42769539e-04 1.34450293e-05\n",
            "   4.17966148e-05 3.41359555e-05 7.06000312e-04 1.92947628e-05\n",
            "   4.74016233e-05 5.33839921e-05 2.16858243e-05 5.09072124e-05\n",
            "   3.84136656e-05 4.94235683e-05 7.20680182e-05 1.75826644e-05\n",
            "   1.11210888e-04 1.13037429e-04 1.09643348e-04 4.92015970e-05\n",
            "   2.30149781e-05 5.09998317e-05 8.83653702e-05 6.67178392e-05\n",
            "   3.35102186e-06 1.27836165e-06 1.49351690e-05 1.40283119e-05\n",
            "   8.95756384e-05 1.54491081e-05 1.04128587e-04 1.66032914e-04\n",
            "   1.47720057e-04 1.81505766e-05 6.44068086e-06 3.05069916e-05\n",
            "   5.64049042e-05 1.59723004e-06 4.23822621e-06 1.75711157e-05\n",
            "   1.56284441e-05 2.06641344e-04 8.10355923e-05]\n",
            "  [1.15814304e-03 2.31297945e-05 4.79320579e-06 9.19726954e-05\n",
            "   4.34800722e-06 7.80728783e-07 6.65805564e-05 4.89390914e-06\n",
            "   2.54268853e-05 1.21166144e-04 1.30465355e-06 2.11653605e-05\n",
            "   2.59782246e-04 1.58149014e-05 7.26251601e-05 3.46075045e-04\n",
            "   5.48814160e-06 1.37472176e-04 1.58094670e-04 7.71803798e-07\n",
            "   5.70955171e-06 8.59276915e-05 2.28467655e-07 7.71651539e-05\n",
            "   1.31000343e-04 3.49796778e-07 9.95303631e-01 4.74682838e-06\n",
            "   6.30278882e-05 5.89857245e-06 1.12038128e-06 3.78145123e-05\n",
            "   3.42089101e-04 1.31514184e-06 1.34916229e-08 3.48309150e-05\n",
            "   1.04459116e-06 3.45064072e-05 1.86913644e-06 7.41871645e-06\n",
            "   1.93782453e-05 1.11293280e-03 4.06533809e-06 2.58139062e-05\n",
            "   7.76692985e-08 4.20006607e-08 3.56084769e-07 2.12550901e-07\n",
            "   2.84282305e-06 1.51760878e-06 1.73211607e-04]\n",
            "  [2.40841086e-06 6.99716338e-08 3.30872081e-06 1.13340548e-06\n",
            "   4.32171328e-05 8.80145581e-06 1.54161520e-04 6.49309732e-06\n",
            "   1.12835914e-06 7.69849237e-07 8.47356787e-05 3.67667008e-06\n",
            "   1.50884416e-05 4.80783967e-07 3.53971927e-06 1.96342153e-05\n",
            "   2.97365732e-05 4.95144377e-07 8.75559363e-06 7.53171189e-05\n",
            "   7.01860330e-08 2.83522212e-07 1.06464347e-06 2.20338112e-08\n",
            "   4.55727047e-07 5.01941322e-05 1.10291580e-06 9.98720944e-01\n",
            "   9.58320470e-07 3.99690066e-07 6.67124141e-06 9.11050861e-08\n",
            "   8.28185075e-05 2.67268369e-05 8.72307240e-08 9.31407840e-08\n",
            "   9.60272246e-06 4.74259537e-07 8.32897331e-06 1.55092512e-05\n",
            "   6.92256663e-06 1.24879698e-05 4.82919451e-04 2.96014349e-07\n",
            "   4.71713720e-05 5.07895857e-05 1.12270840e-07 1.34604727e-06\n",
            "   1.87145815e-07 8.92629851e-06 3.29169971e-08]\n",
            "  [1.70432642e-07 2.08761019e-04 2.25213785e-06 1.19964552e-05\n",
            "   3.46390843e-05 4.87494045e-10 5.85617563e-08 2.64177779e-05\n",
            "   3.13463011e-06 1.90825745e-06 5.09404401e-07 5.40846704e-05\n",
            "   8.51510418e-10 1.42344172e-04 9.15360124e-06 7.67936399e-06\n",
            "   2.96492126e-05 4.31939134e-06 1.86437781e-08 1.59102592e-06\n",
            "   8.18535955e-06 5.90161129e-08 3.89059387e-05 4.40502248e-07\n",
            "   3.27714133e-06 6.13578595e-06 3.04447872e-06 5.27338635e-08\n",
            "   9.99061048e-01 2.36178857e-05 3.48341356e-09 4.74717090e-05\n",
            "   1.57209485e-07 1.28571250e-04 2.29352372e-06 3.21302139e-07\n",
            "   3.15365511e-08 8.64350113e-06 3.82154781e-07 6.66949518e-07\n",
            "   9.56450847e-08 1.87511323e-05 2.24688520e-05 1.18627042e-06\n",
            "   2.12594912e-07 4.11111785e-08 8.09656995e-05 4.89535834e-09\n",
            "   8.93302968e-07 5.15741938e-09 3.38680707e-06]\n",
            "  [1.79430788e-06 1.89575405e-06 8.48776563e-06 5.15947613e-05\n",
            "   2.03765822e-08 1.24991857e-06 5.37377218e-07 9.97696579e-01\n",
            "   5.77237415e-07 2.18066202e-06 4.30356067e-06 1.82755923e-06\n",
            "   3.90777495e-06 3.31638912e-05 4.69078820e-08 3.85017134e-04\n",
            "   1.77271693e-04 1.92029074e-05 4.48127576e-05 3.50035378e-07\n",
            "   2.71868976e-05 2.98053173e-08 3.50446925e-07 5.95445053e-05\n",
            "   1.23358239e-08 3.19527248e-06 1.99740896e-07 3.19678584e-05\n",
            "   1.61881883e-06 3.40937459e-06 9.13711265e-05 3.70482738e-08\n",
            "   1.22874641e-04 1.51112927e-05 2.32727427e-04 3.13202909e-05\n",
            "   5.29063391e-06 5.96283244e-05 1.51931177e-04 3.58502189e-06\n",
            "   3.09158895e-05 5.32233389e-06 1.46277118e-04 1.09342844e-04\n",
            "   1.03058719e-05 8.13554871e-05 1.40057218e-05 3.19583924e-04\n",
            "   3.11768389e-08 6.67705262e-06 6.88135486e-08]\n",
            "  [3.29312002e-07 4.29301319e-04 5.78979234e-06 2.41799917e-05\n",
            "   2.03164109e-06 6.38545998e-06 2.11715853e-10 2.24456630e-06\n",
            "   2.33376923e-07 1.47866686e-07 2.58488967e-06 3.35183358e-06\n",
            "   3.51871137e-07 1.34884630e-07 6.49217668e-07 3.20641966e-08\n",
            "   8.16493139e-06 1.06440479e-04 9.32391060e-07 3.14230374e-06\n",
            "   2.22892083e-07 6.19015964e-07 3.73214704e-08 1.06376795e-07\n",
            "   1.70237513e-07 7.21481968e-07 6.74310002e-07 3.73216693e-07\n",
            "   4.03544218e-05 9.98680770e-01 8.90696413e-07 3.13478668e-05\n",
            "   3.92616982e-07 2.02668398e-05 3.22160759e-06 2.37436601e-04\n",
            "   7.92016692e-07 2.02417763e-08 3.94648305e-05 1.70764524e-05\n",
            "   1.64967230e-06 4.14240139e-06 2.97207151e-07 4.74473964e-05\n",
            "   8.62934539e-05 7.49360879e-07 4.03892300e-05 1.52444363e-06\n",
            "   1.43383702e-04 3.00050687e-08 2.70292321e-06]\n",
            "  [1.62329127e-06 1.78288758e-06 3.10899341e-04 3.61953667e-08\n",
            "   3.46872099e-07 4.22301753e-07 1.60750074e-06 5.30781945e-05\n",
            "   2.33528183e-08 2.88955757e-08 4.97007875e-07 2.47941131e-07\n",
            "   4.62281423e-06 8.70880754e-08 6.81051731e-08 2.33953415e-07\n",
            "   3.45745725e-08 5.12696337e-04 2.86509094e-05 1.69725854e-06\n",
            "   3.48904700e-06 5.03527190e-07 1.00426210e-08 2.77801138e-09\n",
            "   8.50088853e-08 8.53719540e-10 5.99816417e-07 4.21576851e-06\n",
            "   2.64463051e-09 2.61827807e-07 9.98193324e-01 1.01609858e-05\n",
            "   1.20998586e-04 1.22804920e-06 7.13681366e-05 1.15297726e-05\n",
            "   5.84899273e-04 1.15008788e-05 4.20632887e-07 8.62455363e-06\n",
            "   7.23341827e-06 8.25909410e-07 1.49278856e-06 1.03887714e-05\n",
            "   6.83051303e-06 5.01645263e-06 4.65210022e-08 6.70991949e-06\n",
            "   1.88996478e-06 1.77936217e-05 4.54174121e-09]\n",
            "  [1.66685163e-06 2.65069510e-04 2.08679616e-04 2.33367402e-07\n",
            "   3.01875771e-05 6.18883917e-07 2.71233972e-07 3.79843925e-07\n",
            "   6.65359084e-06 7.04361810e-07 1.10746221e-07 2.63506718e-05\n",
            "   4.53633135e-07 2.55437567e-06 3.29215209e-06 1.50480400e-06\n",
            "   6.22103016e-07 8.01885835e-05 7.95504320e-07 1.10433093e-05\n",
            "   2.15409473e-05 6.65864604e-07 2.30597777e-07 2.53294473e-07\n",
            "   1.17204979e-06 2.06863984e-08 9.19651575e-05 7.36779668e-08\n",
            "   1.55360176e-05 2.29808575e-05 6.02774617e-06 9.98769581e-01\n",
            "   1.17978821e-07 2.48506607e-04 3.08809331e-06 1.39763160e-05\n",
            "   1.20171194e-06 9.91996876e-06 3.88015042e-06 5.18571733e-06\n",
            "   1.50316391e-05 5.74969526e-05 1.65375400e-06 1.34740931e-05\n",
            "   1.52474365e-06 1.20895481e-06 4.21023242e-06 1.79297615e-07\n",
            "   4.81240659e-06 1.34357663e-07 4.28550629e-05]]] <class 'numpy.ndarray'> (1, 8, 51)\n",
            "i: 7 Else Y_Predict: [[ 1 26 27 28  7 29 30 31]] (1, 8)\n",
            "Index: 7 Word: played\n",
            "the game that messi and ronaldo have played \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYIAYzdUxBYl",
        "outputId": "c91e386f-f24d-4f0b-ec02-c5a530e3bfc6"
      },
      "source": [
        "# Predicting Probability of Input Sentence\n",
        "\n",
        "def find_probability(model_RNN, courpus_tokenizer, sentence):\n",
        "    enter = courpus_tokenizer.texts_to_sequences([sentence])[0]\n",
        "    enter.insert(0,0)\n",
        "    enter = array(enter)\n",
        "    enter = numpy.reshape(enter, newshape=(1,-1))\n",
        "    print(\"Enter is : \", enter)\n",
        "    print(\"Shape of enter is : \", enter.shape)\n",
        "    probability = model_RNN.predict_proba(enter, verbose=0)\n",
        "    probability_t = 1\n",
        "    for i in range(probability.shape[1]-1):\n",
        "        probability_t = probability_t * probability[0, i,\n",
        "                                                      enter[0, i+1]]\n",
        "    print(\"Probability of sentence - '{}' is {}\".format(sentence, probability_t))\n",
        "\n",
        "while 1:\n",
        "    sentence = input(  \"Please enter your sentence : \" )\n",
        "    find_probability(model, tk, sentence)\n",
        "    choice = input(  \"\\nWould you like to continue? Enter N/n to stop : \" ).lower()\n",
        "    if choice == 'n':\n",
        "        print(\"Done! It is completed!\")\n",
        "        break\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Please enter your sentence : messi and ronaldo\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:425: UserWarning: `model.predict_proba()` is deprecated and will be removed after 2021-01-01. Please use `model.predict()` instead.\n",
            "  warnings.warn('`model.predict_proba()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Enter is :  [[ 0 28  7 29]]\n",
            "Shape of enter is :  (1, 4)\n",
            "Probability of sentence - 'messi and ronaldo' is 6.589099701933851e-15\n",
            "\n",
            "Would you like to continue? Enter N/n to stop : no\n",
            "Please enter your sentence : In modern Football messi\n",
            "Enter is :  [[ 0  2  8  9 28]]\n",
            "Shape of enter is :  (1, 5)\n",
            "Probability of sentence - 'In modern Football messi' is 1.0793117142595449e-10\n",
            "\n",
            "Would you like to continue? Enter N/n to stop : n\n",
            "Done! It is completed!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 546
        },
        "id": "8OwBy9u6xEl7",
        "outputId": "751f27ca-f1cd-438f-b8eb-14fd3da06a5a"
      },
      "source": [
        "# Sentance Generation\n",
        "def sample_sequence_all(model_RNN, courpus_tokenizer, words, size_vocab):\n",
        "  enter = list()\n",
        "  text_input = ''\n",
        "  for i in range(words):\n",
        "    print(\"text_input being taken as '{}'\".format(text_input))\n",
        "    enter = courpus_tokenizer.texts_to_sequences([text_input])[0]\n",
        "    enter.insert(0,0)\n",
        "    enter = array(enter)\n",
        "    enter = numpy.reshape(enter, newshape=(1,-1))\n",
        "    print(\"For i : {} enter is : {}\".format(i, enter))\n",
        "    \n",
        "    if i==0:\n",
        "      probability = model_RNN.predict_proba(enter, verbose=0)\n",
        "      y_predict = 0\n",
        "      while y_predict==0:\n",
        "        y_predict = numpy.random.choice(range(size_vocab), p = probability.ravel())\n",
        "      y_predict = numpy.reshape(numpy.array([y_predict]), newshape = (1,-1))\n",
        "      print(\"For i : {} y_predict in if is : {}\".format(i, y_predict))\n",
        "      \n",
        "    \n",
        "    else:\n",
        "      probability = model_RNN.predict_proba(enter, verbose=0)\n",
        "      y_predict = numpy.append(y_predict,0)\n",
        "      y_predict = numpy.reshape(y_predict, newshape = (1,-1))\n",
        "\n",
        "      while y_predict[0,i]==0:\n",
        "        y_predict[0,i]=numpy.random.choice(range(size_vocab),p=probability[0,i].ravel())\n",
        "      print(\"For i : {} y_predict in else is : {}\".format(i, y_predict))\n",
        "\n",
        "    \n",
        "    exit = ''\n",
        "    for w, index in courpus_tokenizer.word_index.items():\n",
        "      if index == y_predict[0,i]:\n",
        "        exit = w\n",
        "        break\n",
        "    text_input += exit + ' '\n",
        "    print('\\n' + '-'*50 + '\\n')\n",
        "  return text_input\n",
        "\n",
        "\n",
        "\n",
        "totWords = int(input( \"Enter number of total  words to be genrated : \" ))\n",
        "sample_sequence_all(model, tk, totWords , vocab_len)\n",
        "    "
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter number of total  words to be genrated : 4\n",
            "text_input being taken as ''\n",
            "For i : 0 enter is : [[0]]\n",
            "For i : 0 y_predict in if is : [[1]]\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "text_input being taken as 'the '\n",
            "For i : 1 enter is : [[0 1]]\n",
            "For i : 1 y_predict in else is : [[ 1 26]]\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "text_input being taken as 'the game '\n",
            "For i : 2 enter is : [[ 0  1 26]]\n",
            "For i : 2 y_predict in else is : [[ 1 26 27]]\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "text_input being taken as 'the game that '\n",
            "For i : 3 enter is : [[ 0  1 26 27]]\n",
            "For i : 3 y_predict in else is : [[ 1 26 27 28]]\n",
            "\n",
            "--------------------------------------------------\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:425: UserWarning: `model.predict_proba()` is deprecated and will be removed after 2021-01-01. Please use `model.predict()` instead.\n",
            "  warnings.warn('`model.predict_proba()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'the game that messi '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    }
  ]
}